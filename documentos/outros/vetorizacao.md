# **8. Vetoriza√ß√£o**

## 8.1 Bag of Words

### 8.1.1 Introdu√ß√£o

&emsp;&emsp; O modelo _Bag of Words_ √© uma das v√°rias ferramentas de vetoriza√ß√£o de frases e palavras, processo que √© de suma import√¢ncia para o desenvolvimento de um modelo PLN (Processamento de Linguagem Natural), visto que o modelo de _machine learning_ s√≥ pode receber n√∫meros como _inputs_. 

### 8.1.2 M√©todo
&emsp;&emsp; Como √∫ltima etapa de manipula√ß√£o de dados antes do uso do modelo de _Machine Learning_ para a classifica√ß√£o de resultados temos a vetoriza√ß√£o dos coment√°rios, processo que nessa _pipeline_ foi conduzido pelo modelo _Bag of Words (BoW)_. O modelo BoW consiste na elabora√ß√£o de uma matriz a partir de um vocabul√°rio de todos os voc√°bulos presentes nos textos, enquanto que cada linha ser√° um coment√°rio que se deseja vetorizar. √â importante notar que esse modelo √© menos robusto, considerando apenas a frequ√™ncia de palavras em cada frase e n√£o os sentidos sem√¢nticos. <br>
&emsp;&emsp; Para essa etapa, foi utilizada uma inst√¢ncia da classe `CountVectorizer()`, e seus m√©todos, da biblioteca _sklearn_ (scikit-learn) a fim de que fosse gerado um vocabul√°rio e as respectivas correspond√™ncias para cada coment√°rio.

### 8.1.3 Resultados

<img src="https://github.com/2023M6T4-Inteli/Projeto3/blob/main/assets/imagens/bow.jpg">
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Figura 27: Demonstra√ß√£o do Bag of Words
<br>

&emsp;&emsp; Ap√≥s o corpus  dos textos terem passado pelo _pipeline_, chega o momento de analisar as repeti√ß√µes de acordo com cada coment√°rio feito, por meio da t√©cnica _Bag of Words (BoW)_ utilizada em processamento de linguagem natural (PLN). Essa t√©cnica √© utilizada para representar um texto como um conjunto de palavras desordenadas, ignorando a ordem e a estrutura gramatical das frases.  Nesse modelo, cada palavra √∫nica do texto √© transformada em uma _feature_ (caracter√≠stica), e a frequ√™ncia de cada palavra no texto √© usada como um valor num√©rico para a _feature_ correspondente.
<br>
&emsp;&emsp; Por exemplo, a frase "O gato preto pulou o muro" seria representada como um conjunto de palavras desordenadas: `'o', 'gato', 'preto', 'pulou', 'o', 'muro'`. A frequ√™ncia de cada palavra √© contada, e o resultado √© um vetor num√©rico que representa a frequ√™ncia de cada palavra na frase. O modelo _Bag of Words_ √© uma t√©cnica simples e eficiente para representar textos em formato vetorial, o que permite utiliz√°-los em algoritmos de aprendizado de m√°quina. 
<br>
&emsp;&emsp; Assim, abaixo √© poss√≠vel visualizar o c√≥digo necess√°rio para realizar essa vetoriza√ß√£o e o _output_ dele:
```
def bow(comentarios): 
    vectorizer = CountVectorizer(analyzer=lambda x: x)
    bow_model = vectorizer.fit_transform(comentarios)
    bow_df = pd.DataFrame(bow_model.toarray(), columns=vectorizer.get_feature_names_out())
    return bow_df
```
<img src="https://github.com/2023M6T4-Inteli/Projeto3/blob/main/assets/imagens/output.jpg"> 
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Figura 28: Output do c√≥digo
<br>

&emsp;&emsp; Abaixo √© demonstrado um exemplo resultante desta tabela, a qual possui um total de 12.193 linhas, que est√£o de acordo com cada coment√°rio do csv disponibilizado pelo cliente, al√©m de 24.331 colunas, que foram as palavras chaves selecionadas.
```
df['conf'].value_counts() 
0    11795
1      396
2        2
Name: conf, dtype: int64
```
&emsp;&emsp; Neste exemplo, √© poss√≠vel perceber que o termo `‚Äòconf‚Äô`  se repete uma vez, em 396 coment√°rios diferentes, e se repete duas vezes em 2 coment√°rios diferentes. Dessa forma, percebe-se como a fun√ß√£o consegue selecionar palavras chaves que est√£o contidas nas diversas frases do dataframe.

### 8.1.4 Conclus√£o
&emsp;&emsp; Com a aplica√ß√£o do Modelo _Bag of Words (BoW)_ √© poss√≠vel perceber a capacidade de sele√ß√£o de palavras para a futura implementa√ß√£o na _Machine Learning_ desenvolvida. O objetivo do projeto √© demonstrado a partir da imagem abaixo:

<img src="https://github.com/2023M6T4-Inteli/Projeto3/blob/main/assets/imagens/modelo.jpg">
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Figura 29: Demonstra√ß√£o do modelo pronto
<br>

&emsp;&emsp; Por√©m, foi poss√≠vel analisar que √© necess√°rio uma renova√ß√£o no tratamento dos dados e exclus√£o de determinadas palavras, j√° que foi percebido que havia uma alta diversidade de termos que est√£o exclusos e/ou outros que permanecer√£o nas frases e n√£o deveriam permanecer. Abaixo h√° exemplo desta an√°lise:

```
word_counts = df.sum()
top_words = word_counts.sort_values(ascending=False)
top_10 = top_words.head(10)
top_10

btgpactual    6489
invest        4014
btg           2822
tod           1783
banc          1771
sobr          1364
melhor        1363
cont          1332
merc          1305
financeir     1303
dtype: int64
```

&emsp;&emsp; Al√©m disso, foi feita uma _plotagem_ de uma nuvem de palavras para ser mais intuitiva a visualiza√ß√£o dos termos que ser√£o necess√°rios passar por um tratamento.

<img src="https://github.com/2023M6T4-Inteli/Projeto3/blob/main/assets/imagens/nuvem_palavras.png"> 
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Figura 30: Nuvem de palavras
<br>

&emsp;&emsp; Assim, o pr√≥ximo passo √© um retratamento dos textos para ter melhor desenvolvimento e aplica√ß√£o no momento de constru√ß√£o da Intelig√™ncia Artificial.


## 8.2 Word2Vec

### 8.2.1 Introdu√ß√£o
&emsp;&emsp; O modelo de vetoriza√ß√£o Word2Vec, ao contr√°rio do Bag Of Words, permite um entendimento das conex√µes sem√¢nticas das palavras, sendo assim, palavras podem ser analisadas a partir de sua similaridade com outras atrav√©s da ocorr√™ncia contextual. Isso √© poss√≠vel porque o Word2Vec relaciona as palavras a um espa√ßo dimensional, representadas por um vetor denso, onde palavras com similaridade maior tendem a se agrupar no mesmo espa√ßo. O modelo, atrav√©s do uso de uma rede neural embutida, aprende por treinamento em big corpus, como o wikip√©dia, a localiza√ß√£o ideal para cada palavra.

### 8.2.2 M√©todo
&emsp;&emsp;No projeto recorremos a 2 m√©todos de vetoriza√ß√£o com o Word2Vec: o modelo pr√©-treinado de Continuous Bag-of-Words (CBOW) da biblioteca NILC e, posteriormente, o treinado no nosso pr√≥prio corpus. O modelo pr√©-treinado, como dito anteriormente, n√£o necessita de treinamento adicional, pois ele j√° dominou as rela√ß√µes lingu√≠sticas entre as palavras a partir do primeiro treinamento, al√©m disso, o modelo usado √© p√∫blico e permite o uso quase instant√¢neo no corpus. 

### 8.2.3 Resultados
&emsp;&emsp;Para ambos os m√©todos, foi necess√°rio realizar a soma dos vetores de palavras presentes nas frases, desse modo, teremos um vetor para cada frase. No m√©todo pr√©-treinado, constru√≠mos vetores de 50 dimens√µes, enquanto que, no modelo treinado no corpus da base de dados de coment√°rios, constru√≠mos vetores de 100 dimens√µes.

<img src="https://github.com/2023M6T4-Inteli/Projeto3/blob/main/assets/imagens/word2vec_pre_treinado.jpg"> 
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Figura 31: Word2Vec com modelo pr√©-treinado
<br>

<img src="https://github.com/2023M6T4-Inteli/Projeto3/blob/main/assets/imagens/word2vec_corpus.jpg"> 
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Figura 32: Word2Vec treinado com o corpus
<br>

### 8.2.4 Conclus√£o
&emsp;&emsp;A conclus√£o obtida de qual ser√° o melhor m√©todo de vetoriza√ß√£o se dar√° na constru√ß√£o dos modelos de machine learning que ser√° explicado na pr√≥xima sess√£o, pois assim poderemos comparar diretamente a acur√°cia e recall dos diferentes modelos.


## 8.3 TF - IDF

### 8.3.1 Introdu√ß√£o

&emsp;&emsp; O TF-IDF (Term Frequency-Inverse Document Frequency) √© uma t√©cnica que permite avaliar a import√¢ncia relativa de um termo em um documento dentro de um conjunto de documentos. O modelo √© composto por duas partes principais: a frequ√™ncia do termo (TF) e a frequ√™ncia inversa do documento (IDF). A frequ√™ncia do termo mede quantas vezes um termo espec√≠fico aparece em um documento, enquanto a frequ√™ncia inversa do documento mede a raridade do termo em toda a cole√ß√£o de documentos.

### 8.3.2 M√©todo

&emsp;&emsp; O c√≥digo que ser√° apresentado abaixo utiliza o m√©todo TfidfVectorizer() da biblioteca scikit-learn (sklearn) para calcular o TF-IDF dos documentos presentes na coluna 'texto_tratado' de um dataframe chamado 'df'. E no final, o dataframe 'df_final' conter√° todas as colunas do dataframe original, al√©m das colunas correspondentes √†s pontua√ß√µes TF-IDF de cada termo nos documentos.

### 8.3.3 Resultados

&emsp;&emsp; A fun√ß√£o pd.read_csv() √© uma fun√ß√£o da biblioteca pandas (pd) que permite ler dados de um arquivo CSV e retorn√°-los como um dataframe. Ap√≥s a execu√ß√£o do c√≥digo, ao imprimir o 'df', ser√° mostrada uma representa√ß√£o tabular dos dados contidos no arquivo.

```
df = pd.read_csv('caminho_arquivo‚Äô)
df
```

&emsp;&emsp; A primeira linha cria um objeto TfidfVectorizer(), que √© uma classe dispon√≠vel na biblioteca scikit-learn, respons√°vel por transformar textos em uma representa√ß√£o num√©rica usando o c√°lculo do TF-IDF. Em seguida, o m√©todo fit_transform() √© aplicado aos coment√°rios, que j√° passaram pelo pr√© - processamento, presente na coluna 'texto_tratado' do 'df'. Isso transforma os documentos em uma matriz num√©rica esparsa, onde cada linha representa um documento e cada coluna representa um termo ponderado pelo TF-IDF.

```
tfidf_vectorizer = TfidfVectorizer()

vetorizado = tfidf_vectorizer.fit_transform(df['texto_tratado'])
```

&emsp;&emsp; Ap√≥s a vetoriza√ß√£o, a vari√°vel 'feature_names' armazena os termos que foram utilizados na vetoriza√ß√£o, que ser√£o colunas do dataframe resultante. A seguir, a matriz num√©rica esparsa resultante √© convertida em um dataframe chamado 'df_vetorizado', onde cada coluna corresponde a um termo e cada linha representa um documento. Os valores s√£o preenchidos com as pontua√ß√µes TF-IDF.

```
feature_names = tfidf_vectorizer.get_feature_names_out()

df_vetorizado = pd.DataFrame(vetorizado.toarray(), columns=feature_names)
```

&emsp;&emsp; Por √∫ltimo, o 'df' √© concatenado com o dataframe resultante da vetoriza√ß√£o 'df_vetorizado' ao longo do eixo das colunas (axis=1), utilizando a fun√ß√£o concat() da biblioteca pandas. Isso adiciona as colunas com as pontua√ß√µes TF-IDF ao dataframe original, criando assim o dataframe final, chamado de 'df_final'.

```
df_final = pd.concat([df, df_vetorizado], axis=1)

df_final
```

&emsp;&emsp; Para fins de teste, o m√©todo value_counts() √© aplicado para que retorne uma contagem dos valores √∫nicos presentes na coluna especificada. Ele conta quantas vezes cada valor aparece na coluna e retorna os resultados em ordem decrescente, com o valor mais frequente no topo. Somente algumas colunas foram testadas, e as colunas v√£o ser apresentadas abaixo.

```
df_final['ser'].value_counts()

output:
0.000000    8014
0.136855       1
0.089819       1
0.178045       1
0.162049       1
0.147501       1
0.176240       1
0.090406       1
0.073729       1
0.264882       1
0.436092       1
0.064391       1
0.145558       1
0.133492       1
0.405338       1
0.190045       1
0.169664       1
0.314675       1
0.081824       1
0.149980       1
0.066284       1
0.177696       1
0.143129       1
0.478244       1
0.164816       1
0.167999       1
0.120388       1
Name: ser, dtype: int64
```

```
df_final['aa'].value_counts()

output:
0.000000    8039
0.215284       1
Name: aa, dtype: int64
```

```
df_final['ùöúùöéùöûùöú'].value_counts()

output:
0.000000    8039
0.131107       1
Name: ùöúùöéùöûùöú, dtype: int64
```

### 8.3.4 Conclus√£o

&emsp;&emsp; O uso do TF-IDF em conjunto com t√©cnicas de vetoriza√ß√£o e manipula√ß√£o de dados, como apresentado no c√≥digo, √© uma ferramenta valiosa para processamento de texto e an√°lise de dados, fornecendo insights sobre a import√¢ncia relativa dos termos em um conjunto de documentos e permitindo uma melhor compreens√£o e interpreta√ß√£o dos textos.

