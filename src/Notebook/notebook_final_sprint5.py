"""Notebook_Final_Sprint5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uzrZKR2USFaiXyBvjEI-6VFs1ZAG3eRf

#  üî∑ **Projeto Grupo BT-G3**

<center><img src="https://cdn.discordapp.com/attachments/937463646181785633/1100078873422602350/BTG3_1.png" width="50%" height="30%"/></center>

## üöÄ **Integrantes do grupo**
- Daniel Barzilai
- Larissa Carvalho
- Maria Luisa Maia
- Pedro Rezende
- Rafael Moritz
- Vitor Oliveira

<center><img src="https://www.inteli.edu.br/wp-content/uploads/2021/08/20172028/marca_1-2.png" width="50%" height="30%"/></center>

<h1 align='center'><b>IA para Marketing: Monitoramento de campanhas utilizando processamento de linguagem natural (PLN)<b></h1>

<center><img src="https://upload.wikimedia.org/wikipedia/commons/c/c2/Btg-logo-blue.svg" width="50%" height="50%"/></center>

<h2 align='center'>O Banco BTG Pactual enfrenta um desafio na √°rea de Marketing em entender as necessidades e demandas dos clientes de maneira f√°cil e r√°pida nas redes sociais.</h2>

<h2 align='center'>A solu√ß√£o proposta para esse problema foi o desenvolvimento de uma Intelig√™ncia Artificial utilizando processamento de linguagem natural (PLN), capaz de monitorar os sentimentos das campanhas de marketing, voltadas para o Instagram. </h2>

<h2 align='center'> O objetivo principal dessa solu√ß√£o √© acatar os dados, analisar e interpretar as mensagens e coment√°rios enviados pelos clientes na rede social, a fim de identificar as necessidades e demandas de forma precisa e eficiente, a partir do sentimento exposto dentro daquela campanha.</h2>

# ‚ùó **Sobre os dados**

Esse projeto est√° utilizando dados coletados e tratados pela equipe de Automation do banco BTG Pactual, o qual disponibilizou o dataset.

Com base nas informa√ß√µes dispostas nesse dataset, realizamos insights a cerca dos coment√°rios feitos nos posts do Instagram do pr√≥prio banco. Vale lembrar que os dados est√£o anonimizados e resguardados para manter a privacidade e √©tica com os usu√°rios e com o banco.

Agradecemos a compreens√£o ü§ì

# üíª **Sobre o notebook**

O projeto foi realizado atrav√©s do Google Colaboratory / Jupyter Notebook (arquivo IPYNB). Nas sess√µes seguintes iremos demonstrar um passo a passo de como os dados foram manejados at√© chegar em uma conclus√£o e na entrega do modelo. As sess√µes est√£o estruturadas da seguinte forma:

1. Setup do notebook;
2. Explorat√≥ria dos dados;
3. Cria√ß√£o de features;
4. Pr√©-processos dos dados;
5. Modelo de Vetoriza√ß√£o;
5. Constru√ß√£o do Modelo;
6. Interpreta√ß√£o e Avalia√ß√£o do modelo;

**OBS**: O notebook foi desenvolvido com foco em fun√ß√µes, para que fosse poss√≠vel fazer a substitui√ß√£o de qualquer vari√°vel e obter o resultado a partir de uma nova base.

# ‚è∞ Principais mudan√ßas da √∫ltima Sprint

1. Fizemos a escolha de proceder com o processo de vetoriza√ß√£o Word2Vec
2. Escolhemos o algoritmo Random Forest como principal
3. Desenvolvemos este notebook para apresentar como notebook final
4. Desenvolvemos o servi√ßo de mostragem do que foi desenvolvido, a partir da ferramenta Streamlit (est√° em uma pasta do GitHub)
5. Refinamos os hiperpar√¢metros do modelo para buscar um melhor valor de recall. No final, tivemos um modelo de 81% de recall voltado para os negativos, que era o nosso objetivo.

# 1. üîß Setup - Instala√ß√µes

A configura√ß√£o do setup √© o ato de preparar e arrumar o ambiente de maneira apropriada para ser utilizado. Isso inclui a instala√ß√£o de bibliotecas e a realiza√ß√£o de ajustes necess√°rios.

O prop√≥sito √© criar um ambiente operacional no qual seja poss√≠vel realizar tarefas espec√≠ficas com efici√™ncia.

## 1.1 Conex√£o com o Google Drive (se necess√°rio)
"""

#Conectar com o Google Drive - Para casos de uso do Google Colaboratory

from google.colab import drive
drive.mount('/content/drive')

"""## 1.2 Instala√ß√£o e Importa√ß√£o de bibliotecas"""

!pip install unidecode

!pip install emoji

!pip install -U spacy
!python -m spacy download pt_core_news_sm

!pip install nltk

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import unidecode
import seaborn as sns
from collections import Counter

import emoji
import spacy
import nltk

nltk.download('punkt')
from nltk import tokenize
from nltk.tokenize import TweetTokenizer
from nltk import word_tokenize, pos_tag

nltk.download('stopwords')
from nltk.corpus import stopwords

nltk.download('rslp')
from nltk.tokenize import RegexpTokenizer

nltk.download('wordnet')
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import PlaintextCorpusReader
from nltk.stem import WordNetLemmatizer

from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from sklearn import preprocessing
from sklearn import metrics
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report, confusion_matrix, f1_score, recall_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample

import gensim
from scipy.spatial.distance import cosine
from gensim.models import KeyedVectors, Word2Vec

import pickle

"""# 2. üåé Explorat√≥ria dos dados

## 2.1 Leitura e visualiza√ß√£o do CSV

- Explicar o porque de eu estar tirando a coluna autor
"""

df = pd.read_csv('/content/drive/MyDrive/MoÃÅdulo 6/projeto/Bases/base_nova_csv.csv')
df_original = df.drop(columns=['"autor"'])
df_original

df_original.columns

df_original.dtypes

"""## 2.2 Tratamento da base

### 2.2.1 Retirada de " " (aspas) das colunas
"""

def tiraAspas(base):
    df = base.rename(columns=lambda x: re.sub('[\"\']', '', x))
    return df

df_tratada = tiraAspas(df_original)
df_tratada

df_tratada['sentimento'].value_counts()

df_tratada['tipoInteracao'].value_counts()

df_tratada['anomalia'].value_counts()

df_tratada['contemHyperlink'].value_counts()

df_tratada['processado'].value_counts()

"""### 2.2.2 Drop nas linhas com **Anomalia**

- Explicar o que √© Anomalia e as outras colunas que influenciam nela
"""

df_sem_anomalia = df_tratada[df_tratada['anomalia'] != 1]
df_sem_anomalia

"""### 2.2.3 Drop nas colunas que n√£o nos servem"""

df_dropado = df_sem_anomalia.drop(['id', 'dataPublicada', 'anomalia', 'probabilidadeAnomalia', 'linkPost', 'processado', 'contemHyperlink'], axis=1)
df_dropado

df_tratada['sentimento'].value_counts()

df = df_dropado.reset_index(drop=True)
df

"""## 2.3 Plotando Gr√°ficos

#### Os tipos de intera√ß√µes mais frequentes
"""

# Contar os tipos de intera√ß√£o
count_interactions = df['tipoInteracao'].value_counts()
# Criar o gr√°fico de pizza
plt.figure(figsize=(8, 6))
count_interactions.plot(kind='pie', autopct='%1.1f%%')
plt.title('Tipos de Intera√ß√£o')
plt.ylabel('')
plt.show()

"""#### Tipos de sentimentos mais frequentes"""

# Contar os tipos de sentimentos
count_sentimentos = df['sentimento'].value_counts()

# Plotagem do gr√°fico de barras
plt.figure(figsize=(10, 6))
count_sentimentos.plot(kind='bar')
plt.xlabel('Sentimentos')
plt.ylabel('Contagem')
plt.title('Tipos de Sentimento')
plt.show()

"""#### Frequ√™ncia dos sentimentos por tipo de Intera√ß√£o"""

# Contar os tipos de sentimentos por Intera√ß√£o
contagem = df.groupby(['tipoInteracao', 'sentimento']).size().unstack(fill_value=0)

# Plotagem do gr√°fico de barras
plt.figure(figsize=(10, 6))
contagem.plot(kind='bar', stacked=True)
plt.xlabel('Sentimentos por Intera√ß√£o')
plt.ylabel('Contagem')
plt.title('Tipos de Sentimento')
plt.show()

"""# 3. üîç Cria√ß√£o de features

- Nesta se√ß√£o iremos criar as features que ir√£o ajudar o nosso modelo a analisar novas frases e descobrir o sentimento por tr√°s delas.
- Essas features ser√£o uma lista de emojis feita a m√£o por n√≥s!
- A lista pode ser modificada, tanto em sua quantidade, quanto em seus valores textuais.
"""

df

"""## 3.1 Emojis presentes no dataset"""

# Cria um conjunto vazio para armazenar os emojis encontrados
emojis_encontrados = set()

# Define uma express√£o regular para encontrar emojis
emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # s√≠mbolos e pictogramas
                           u"\U0001F680-\U0001F6FF"  # transporte e s√≠mbolos de mapas
                           u"\U0001F1E0-\U0001F1FF"  # bandeiras do mundo
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)

# Itera sobre cada linha do dataframe e extrai os emojis da coluna 'texto'
for text in df['texto']:
    emojis_encontrados.update(set(emoji_pattern.findall(text)))

# Converte o conjunto de emojis encontrados em uma lista
lista_de_emojis = list(emojis_encontrados)

# Cria uma lista vazia para armazenar os emojis separados
emojis_separados = []

# Itera sobre a lista de emojis e adiciona cada caracter a uma nova lista
for emoji in lista_de_emojis:
    emojis_separados.extend(list(emoji))

# Exibe a lista de emojis separados
print('Lista de emojis separados:')
print(emojis_separados)

# Conta a frequ√™ncia de cada emoji na lista de emojis separados
frequencia_de_emojis = Counter(emojis_separados)

# Exibe a frequ√™ncia de cada emoji
print('Frequ√™ncia de cada emoji:')
for emoji, frequencia in frequencia_de_emojis.items():
    print(f'{emoji}: {frequencia}')

"""## 3.2 Frequ√™ncia dos emojis"""

def frequencia(input):
    emojis = frequencia_de_emojis
    dfCount = pd.DataFrame(list(emojis.items())) # transformando em dataframe para melhor visualiza√ß√£o
    dfCount.rename(columns={0: "Emoji", 1:"Frequ√™ncia"}, inplace=True) # renomeando as colunas
    final_df = dfCount.sort_values(by=['Frequ√™ncia'], ascending=False) # ordenando o dataframe
    return final_df

output = frequencia(df['texto'])
output.head(10)

"""## 3.3 Cria√ß√£o das colunas/features com valor textual dos emojis

"""

emoji_dict = {
    'üòÑ': 'sorriso',
    'üòÜ': 'risos',
    'üòä': 'envergonhado',
    'üòÉ': 'sorridente',
    'üòè': 'sorriso',
    'üòç': 'olhos_coracao',
    'üòò': 'beijo_coracao',
    'üòö': 'beijo_olhos_fechados',
    'üò≥': 'corado',
    'üòå': 'aliviado',
    'üòâ': 'satisfeito',
    'üòÅ': 'sorrisao',
    'üòú': 'piscadela',
    'üòù': 'lingua_fora_olhos_fechados',
    'üòÄ': 'rindo_muito',
    'üòó': 'beijo',
    'üòô': 'beijo_sorriso_olhos',
    'üòõ': 'lingua_fora',
    'üò¥': 'dormindo',
    'üòü': 'preocupado',
    'üò¶': 'franzindo_testa',
    'üòß': 'angustiado',
    'üòÆ': 'boca_aberta',
    'üò¨': 'careta',
    'üòï': 'confuso',
    'üòØ': 'silencioso',
    'üòë': 'express√£o_neutra',
    'üòí': 'desinteressado',
    'üòÖ': 'sorriso_suado',
    'üòî': 'desapontado',
    'üò®': 'amedrontado',
    'üò∞': 'suando_frio',
    'üò£': 'perseverando',
    'üò¢': 'choro',
    'üò≠': 'choro',
    'üòÇ': 'alegria',
    'üò≤': 'surpreso',
    'ü§ì': 'nerd',
    'üò´': 'cansado',
    'üò†': 'raiva',
    'üò°': 'raiva_intensa',
    'üò§': 'bufada',
    'üòã': 'delicioso',
    'üò∑': 'mascara',
    'üòé': 'oculos_escuros',
    'üòµ': 'morte',
    'üëø': 'diabinho',
    'üòà': 'diabinho',
    'üò∂': 'sem_boca',
    'üíô': 'cora√ß√£o',
    '‚ú®': 'brilho',
    'üåü': 'estrela brilhante',
    'üí•': 'explosao',
    '‚ùó': 'exclamacao',
    '‚ùì': 'interrogacao',
    'üí§': 'sono',
    'üî•': 'fogo',
    'üí©': 'coc√¥',
    'üëå': 'ok',
    'üëä': 'soquinho',
    'üëã': 'aceno',
    'üôå': 'maos_ceu',
    'üôè': 'rezando',
    'üëè': 'palmas',
    'üí™': 'for√ßa',
    'üñï': 'dedo_meio',
    'üëâ': 'apontando_direita',
    'üôà': 'vergonha',
    'üíã': 'beijo',
    'üéâ': 'celebra√ß√£o',
    'üéä': 'confete',
    'üìû': 'telefone',
    'üì£': 'anuncio',
    '‚åõ': 'tempo_acabando',
    '‚è∞': 'alarme',
    'üí∞': 'grana',
    'üí£': 'bomba',
    'üìà': 'gr√°fico_subindo',
    'üìâ': 'gr√°fico_descendo',
    'üìÖ': 'calend√°rio',
    'üè¶': 'banco',
    'üöÄ': 'foguete',
    'üö®': 'sirene',
    'üîù': 'top',
    'üÜô': 'up',
    'üÜò': 'socorro',
    'üîû': 'proibido_menores_idade',
    'üîú': 'breve',
    '‚ùå': 'erro',
    'üíØ': 'perfei√ß√£o',
    '‚úÖ': 'aprovado',
    '‚§µÔ∏è': 'seta_para_baixo_esquerda',
    '‚ö†Ô∏è': 'aten√ß√£o',
    'ü§Ø': 'cabe√ßa_explodindo',
    'üí∏': 'dinheiro',
    'üìç': 'localiza√ß√£o',
    'üíï': 'localiza√ß√£o',
    'üçÄ': "trevo",
    'üèÜ': "trofeu" ,
    'üíñ': "coracao_pixel" ,
    'üëΩ': "alien" ,
    'üéØ': "alvo",
    'üìä': "grafico_barra",
    'üëé': "ruim",
    'üòê': "entediado",
    'üòá': "anjo" ,
    'üëº': "anjo",
    'üíµ': "dinheiro",
    'üí¥': "dinheiro",
    'üí≥': "cartao",
    'üíª': "computador",
    'üì∑': "camera",
    'üëç': "bom",
    'üòû': "decepcao",
    'üíö': "coracao",
    'üö´': "bloqueado",
    'üí≤': "dinheiro",
    'üåπ': "flor",
    'üëÄ': "olhos",
    'üëà': "mao_esquerda",
    'üíì': "coracao",
    'üíú': "coracao",
    'üôÉ': "cara_baixo",
    'üëª': "fantasma",
    'üëá': "dedo_baixo",
    'üò±': "assustado",
    'üò©': "nervoso",
    'üíõ': "coracao",
    'üòª': "apaixonado",
    'üò∫': "feliz",
    'ü§í': "doente"
}

len(emoji_dict)

# Fun√ß√£o para verificar a exist√™ncia de cada emoji no texto
def verifica_emoji(texto, emoji):
    if emoji in texto:
        return 1
    else:
        return 0

# Adiciona uma coluna para cada emoji no DataFrame
for emoji, descricao in emoji_dict.items():
    df[descricao] = df['texto'].apply(lambda x: verifica_emoji(x, emoji))

# Exibe o DataFrame resultante
df

df['palmas'].value_counts()

"""# 4. ‚åõ Pr√©-Processamento dos dados

Aqui em Pr√©-processamento, faremos a defini√ß√£o de diversas fun√ß√µes referentes aos tratamentos que mostramos acima!

Assim, nosso objetivo aqui √© simplificar os as frases, separando em palavras, alterando o valor sem√¢ntico, tratando abrevia√ß√µes, etc., com o objetivo de fazer as frases mais simpl√≥rias e deixar a vetoriza√ß√£o mais "assertiva", resultando numa melhor an√°lise do modelo.

essa √© a lista de tratamentos que iremos fazer em nosso *corpus*:

1. Tokeniza√ß√£o
2. Tratamento de emojis
3. Remo√ß√£o de Alfanum√©ricos
4. Tratamento de abrevia√ß√µes
5. Remo√ß√£o de Stopwords
6. Lematiza√ß√£o

- Retirando a coluna de 'TipoInteracao' pois n√£o iremos usar mais
"""

df = df.drop(['tipoInteracao'], axis=1)
df

"""## 4.1 Cria√ß√£o das Fun√ß√µes

### 4.1.1 Tokeniza√ß√£o
"""

def tokenizer(comentarios):
  comentarios_tokenizados = []
  tk = TweetTokenizer()
  for comentario in comentarios:
    palavras = tk.tokenize(comentario.lower())
    comentarios_tokenizados.append(palavras)
  return comentarios_tokenizados

"""### 4.1.2 Tratamento de emojis"""

emoji_dict = {
    'üòÑ': 'sorriso',
    'üòÜ': 'risos',
    'üòä': 'envergonhado',
    'üòÉ': 'sorridente',
    'üòè': 'sorriso',
    'üòç': 'olhos_coracao',
    'üòò': 'beijo_coracao',
    'üòö': 'beijo_olhos_fechados',
    'üò≥': 'corado',
    'üòå': 'aliviado',
    'üòâ': 'satisfeito',
    'üòÅ': 'sorrisao',
    'üòú': 'piscadela',
    'üòù': 'lingua_fora_olhos_fechados',
    'üòÄ': 'rindo_muito',
    'üòó': 'beijo',
    'üòô': 'beijo_sorriso_olhos',
    'üòõ': 'lingua_fora',
    'üò¥': 'dormindo',
    'üòü': 'preocupado',
    'üò¶': 'franzindo_testa',
    'üòß': 'angustiado',
    'üòÆ': 'boca_aberta',
    'üò¨': 'careta',
    'üòï': 'confuso',
    'üòØ': 'silencioso',
    'üòë': 'express√£o_neutra',
    'üòí': 'desinteressado',
    'üòÖ': 'sorriso_suado',
    'üòî': 'desapontado',
    'üò®': 'amedrontado',
    'üò∞': 'suando_frio',
    'üò£': 'perseverando',
    'üò¢': 'choro',
    'üò≠': 'choro',
    'üòÇ': 'alegria',
    'üò≤': 'surpreso',
    'ü§ì': 'nerd',
    'üò´': 'cansado',
    'üò†': 'raiva',
    'üò°': 'raiva_intensa',
    'üò§': 'bufada',
    'üòã': 'delicioso',
    'üò∑': 'mascara',
    'üòé': 'oculos_escuros',
    'üòµ': 'morte',
    'üëø': 'diabinho',
    'üòà': 'diabinho',
    'üò∂': 'sem_boca',
    'üíô': 'cora√ß√£o',
    '‚ú®': 'brilho',
    'üåü': 'estrela brilhante',
    'üí•': 'explosao',
    '‚ùó': 'exclamacao',
    '‚ùì': 'interrogacao',
    'üí§': 'sono',
    'üî•': 'fogo',
    'üí©': 'coc√¥',
    'üëå': 'ok',
    'üëä': 'soquinho',
    'üëã': 'aceno',
    'üôå': 'maos_ceu',
    'üôè': 'rezando',
    'üëè': 'palmas',
    'üí™': 'for√ßa',
    'üñï': 'dedo_meio',
    'üëâ': 'apontando_direita',
    'üôà': 'vergonha',
    'üíã': 'beijo',
    'üéâ': 'celebra√ß√£o',
    'üéä': 'confete',
    'üìû': 'telefone',
    'üì£': 'anuncio',
    '‚åõ': 'tempo_acabando',
    '‚è∞': 'alarme',
    'üí∞': 'grana',
    'üí£': 'bomba',
    'üìà': 'gr√°fico_subindo',
    'üìâ': 'gr√°fico_descendo',
    'üìÖ': 'calend√°rio',
    'üè¶': 'banco',
    'üöÄ': 'foguete',
    'üö®': 'sirene',
    'üîù': 'top',
    'üÜô': 'up',
    'üÜò': 'socorro',
    'üîû': 'proibido_menores_idade',
    'üîú': 'breve',
    '‚ùå': 'erro',
    'üíØ': 'perfei√ß√£o',
    '‚úÖ': 'aprovado',
    '‚§µÔ∏è': 'seta_para_baixo_esquerda',
    '‚ö†Ô∏è': 'aten√ß√£o',
    'ü§Ø': 'cabe√ßa_explodindo',
    'üí∏': 'dinheiro',
    'üìç': 'localiza√ß√£o',
    'üíï': 'localiza√ß√£o',
    'üçÄ': "trevo",
    'üèÜ': "trofeu" ,
    'üíñ': "coracao_pixel" ,
    'üëΩ': "alien" ,
    'üéØ': "alvo",
    'üìä': "grafico_barra",
    'üëé': "ruim",
    'üòê': "entediado",
    'üòá': "anjo" ,
    'üëº': "anjo",
    'üíµ': "dinheiro",
    'üí¥': "dinheiro",
    'üí≥': "cartao",
    'üíª': "computador",
    'üì∑': "camera",
    'üëç': "bom",
    'üòû': "decepcao",
    'üíö': "coracao",
    'üö´': "bloqueado",
    'üí≤': "dinheiro",
    'üåπ': "flor",
    'üëÄ': "olhos",
    'üëà': "mao_esquerda",
    'üíì': "coracao",
    'üíú': "coracao",
    'üôÉ': "cara_baixo",
    'üëª': "fantasma",
    'üëá': "dedo_baixo",
    'üò±': "assustado",
    'üò©': "nervoso",
    'üíõ': "coracao",
    'üòª': "apaixonado",
    'üò∫': "feliz",
    'ü§í': "doente"
}

def demojize_tokens(words, emoji_dict):
    demojized = []
    for word in words:
        if isinstance(word, list):  # Verifica se √© uma lista de tokens
            processed_word = []
            for token in word:
                if emoji.emoji_count(token) > 0:
                    if token in emoji_dict:
                        processed_word.append(emoji_dict[token])
                    else:
                        processed_word.append(emoji.demojize(token))
                else:
                    processed_word.append(token)
            demojized.append(processed_word)
        else:
            if emoji.emoji_count(word) > 0:
                if word in emoji_dict:
                    demojized.append(emoji_dict[word])
                else:
                    demojized.append(emoji.demojize(word))
            else:
                demojized.append(word)

    demojized = [
        [
            token.replace(":", "").replace("_", "") if any(c in token for c in [":", "_"]) else token
            for token in sublist
        ] if isinstance(sublist, list)
        else sublist.replace("-", "_") if "-" in sublist
        else sublist
        for sublist in demojized
    ]

    return demojized

"""### 4.1.3 Tratamento ReGex"""

def removendo_alfanumericos(tokens):
  output_tokens = []
  for sentence in tokens:
      output_list = []
      for palavra in sentence:
          if palavra.strip(): # Verifica se a palavra n√£o √© uma string vazia
            palavra_sem_marcacao = re.sub((r'@\w*'), '', palavra)
            palavra_sem_hashtag = re.sub((r'#\w*'), '', palavra_sem_marcacao)
            palavra_sem_hyperlink = re.sub(r'https\S*', '', palavra_sem_hashtag)
            palavra_sem_www = re.sub(r'\bwww\.[^\s]*', '', palavra_sem_hyperlink)
            palavra_sem_numeros = re.sub((r'[0-9]'), '', palavra_sem_www)
            palavra_sem_btg = re.sub((r'\bbtg\b'), '', palavra_sem_numeros)
            palavra_sem_btgpactual = re.sub((r'\bpactual\b'), '', palavra_sem_btg)
            output_list.extend(re.findall(r'\w+', palavra_sem_btgpactual)) # analisar se n√£o √© melhor usar o append em vez de extend
      output_tokens.append(output_list)
  return output_tokens

"""### 4.1.4 Tratamento de abrevia√ß√µes"""

# Dicion√°rio de g√≠rias e abrevia√ß√µes para normaliza√ß√£o
dicionario_girias = {
    'vc': 'voc√™',
    'vcs':'voc√™',
    'Vc': 'voc√™',
    'pq': 'porque',
    'Pq': 'porque',
    'tbm': 'tamb√©m',
    'q': 'que',
    'td': 'tudo',
    'blz': 'beleza',
    'flw': 'falou',
    'kd': 'cad√™',
    'Gnt ': 'gente',
    'gnt ': 'gente',
    'to': 'estou',
    'mt': 'muito',
    'cmg': 'comigo',
    'ctz': 'certeza',
    'jah': 'j√°',
    'naum': 'n√£o',
    'ta': 'est√°',
    'eh': '√©',
    'vdd': 'verdade',
    'vlw': 'valeu',
    'p': 'para',
    'sdds': 'saudades',
    'qnd': 'quando',
    'msm': 'mesmo',
    'fzr': 'fazer',
    's' : 'sim',
    'ss': 'sim',
    'Ss': 'sim',
    'pdc': 'pode crer',
    'n' : 'n√£o',
    'nn': 'n√£o',
    'Nn': 'n√£o',
    'pls': 'please',
    'obg': 'obrigado',
    'agr': 'agora'
}

def comentarios_normalizados(tokens, dicionario_girias):
  tokens_normalizados = []

  for sentence in tokens:
    treated = []

    for palavra in sentence:
      if palavra in dicionario_girias:
          palavra_normalizada = dicionario_girias.get(palavra, palavra)
          treated.append(palavra_normalizada)
      else:
          treated.append(palavra)

    treated = [palavra.replace(' ', '') if '_' in palavra else palavra for palavra in treated]
    tokens_normalizados.append(treated)

  return tokens_normalizados

"""### 4.1.5 Remo√ß√£o de Stopwords"""

stopwords = nltk.corpus.stopwords.words('portuguese')
len(stopwords)

type(stopwords)

new_stopwords = [ 'a', '√†', 'a√≠', 'ali', 'ante', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'as', '√†s', 'c√°', 'catorze', 'cento', 'cima', 'cinco', 'coisa', 'coisas', 'com', 'da', 'd√°', 'daquela', 'daquelas', 'daquele', 'daqueles', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'desde', 'dessa', 'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'destes', 'disse', 'disso', 'disto', 'do', 'dois', 'dos', 'e', '√©', 'ela', 'elas', 'ele', 'eles', 'em', 'era', 'eram', '√©s', 'essa', 'essas', 'esse', 'esses', 'esta', 'est√°s', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiv√©ramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estiv√©ssemos', 'estiveste', 'estivestes', 'fazeis', 'fazes', 'for', 'f√¥ramos', 'forem', 'formos', 'f√¥ssemos', 'foste', 'hajam', 'hajamos', 'h√£o', 'havemos', 'havia', 'hei', 'houvemos', 'houver', 'houvera', 'houver√°', 'houveram', 'houv√©ramos', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houver√≠amos', 'houvermos', 'houvesse', 'houvessem', 'houv√©ssemos', 'la','lhe', 'lhes', 'lo', 'na', 'nas', 'o', 'os', 'ou', 'partir', 'paucas', 'per', 'perante', 'p√¥de', 'qu√°is', 'que', 'quereis', 'queremas', 'queres', 'sob', 'sois', 'teus', 'ti', 'tivera', 'tiv√©ramos', 'tiverem', 'tivermos', 'tiveste', 'tivestes', 'vais', 'vossas', 'vosso', 'vossos', 'zero', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '_' ]

len(new_stopwords)

sem_acentuacao_stopwords = ['de','a','o','que','e','do','da','em','um','e','os','dos','ele','das','a','ou','ser','ha','esta','nas','esse','suas','elas','lhe','pelas','te','vos','lhes','tua','teus','tuas','aquelas','estiveramos','hei','ha','havemos','hao','houve','houvemos','houveramos','hajamos','hajam','houvesse','houvessemos','houvessem','houver','houvermos','houverem','houverei','houvera','houveremos','houverao','houveria','houveriamos','houveriam','sao','foramos','formos','tivera','tiverem']
len(sem_acentuacao_stopwords)

def merge_stopwords(arr1, arr2):
    merged = arr1.copy()  # Cria uma c√≥pia do primeiro array
    for element in arr2:
        if element not in merged:
            merged.append(element)  # Adiciona apenas os elementos que n√£o est√£o presentes no primeiro array
    return merged

stopwords = merge_stopwords(stopwords, new_stopwords)
stopwords = merge_stopwords(stopwords, sem_acentuacao_stopwords)
stopwords.remove('n√£o')

len(stopwords)

type(stopwords)

def remove_stopwords(tokens):
  filtered_tokens = []
  for sentence in tokens:
      filtered = [palavra for palavra in sentence if palavra not in stopwords]
      filtered_tokens.append(filtered)
  return filtered_tokens

"""### 4.1.6 Lematiza√ß√£o"""

def lematizacao(tokens):
  # Carregar o modelo pr√©-treinado do SpaCy para o idioma portugu√™s
  nlp = spacy.load("pt_core_news_sm")
  lemmatized_tokens = []

  for sentence in tokens:
    lemma_list = []
    doc = nlp(" ".join(sentence))  # Unir as palavras da frase em uma √∫nica string

    for token in doc:
      if token.lemma_ != '-PRON-':
        if token.pos_ == 'VERB':
          palavra_lematizada = token.lemma_
        else:
          palavra_lematizada = token.lemma_

        if palavra_lematizada:
          lemma_list.append(palavra_lematizada)

    lemmatized_tokens.append(lemma_list)

  # Converter todas as palavras para min√∫sculas
  lemmatized_tokens_lower = []
  for sentence in lemmatized_tokens:
    sentence_lower = [palavra.lower() for palavra in sentence]
    lemmatized_tokens_lower.append(sentence_lower)

  return lemmatized_tokens_lower

"""## 4.2 Testes das Fun√ß√µes

### Printando frase de exemplo, para teste
"""

print(df['texto'].iloc[92])

"""### 4.2.1 Tokeniza√ß√£o"""

tokenized_data = tokenizer(df['texto'])
print(tokenized_data[92])

"""### 4.2.2 Tratamento de emojis
‚ùó‚ùó‚ùó**AVISO IMPORTANTE**‚ùó‚ùó‚ùó

Caso n√£o funcione, volte para a se√ß√£o '**1.2 Instala√ß√£o e Importa√ß√£o de bibliotecas**' e coloque para rodar **TUDO** novamente.
"""

demojized_data = demojize_tokens(tokenized_data, emoji_dict)
print(demojized_data[92])

"""### 4.2.3 Tratamento ReGex"""

alfanumericos_data = removendo_alfanumericos(demojized_data)
print(alfanumericos_data[92])

"""### 4.2.4 Tratamento de abrevia√ß√µes"""

normalizacao_data = comentarios_normalizados(alfanumericos_data, dicionario_girias)
print(normalizacao_data[92])

"""### 4.2.5 Remo√ß√£o de Stopwords
‚ùó‚ùó‚ùó**AVISO IMPORTANTE**‚ùó‚ùó‚ùó

Caso n√£o funcione, volte para a se√ß√£o '**4.1.5 Remo√ß√£o de Stopwords**' e coloque para rodar **TUDO** novamente.
"""

stopwords_data = remove_stopwords(normalizacao_data)
print(stopwords_data[92])

"""### 4.2.6 Lematiza√ß√£o"""

lematizacao_data = lematizacao(stopwords_data)
print(lematizacao_data[92])

"""## 4.3 Criando novas colunas

Nessa se√ß√£o de processamento, n√≥s aplicamos as fun√ß√µes criadas anteriormente e passamos elas no dataframe, como forma de demonstrar os resultados efetivos.

- tokenized = Tokeniza√ß√£o das frases
- emojis_tratados = tratamento de emojis
- regex_tratado = tratamento ReGex
- sem_abreviacoes = Tratamento de abrevia√ß√µes
- no_stopwords = tratamento de stopwords
- lematizado = Lematiza√ß√£o do conte√∫do
"""

df

"""### 4.3.1 Tokeniza√ß√£o"""

df['tokenized'] = tokenizer(df['texto'])
df.head(10)

"""### 4.3.2 Tratamento de emojis"""

df['emojis_tratados'] = demojize_tokens(df['tokenized'], emoji_dict)
df.head(10)

"""### 4.3.3 Tratamento ReGex"""

df['regex_tratado'] = removendo_alfanumericos(df['emojis_tratados'])
df.head(10)

"""### 4.3.4 Tratamento de abrevia√ß√µes"""

df['sem_abreviacoes'] = comentarios_normalizados(df['regex_tratado'],dicionario_girias) # tem que ver como passar como argumento o dicion√°rio de abrevia√ß√£o e a lista de excluir palavras
df.head(10)

"""### 4.3.5 Remo√ß√£o de Stopwords"""

df['no_stopwords'] = remove_stopwords(df['sem_abreviacoes'])
df.head(10)

"""### 4.3.6 Lematiza√ß√£o"""

df['lematizado'] = lematizacao(df['no_stopwords'])
df

"""### 4.3.7 Organiza√ß√£o do dataset

- Drop em linhas nulas
"""

df = df.drop(df[df['lematizado'].apply(lambda x: len(x) == 0)].index)
df

"""- Resetando o index"""

df = df.reset_index(drop=True)
df

"""- Drop em colunas que n√£o ser√£o mais utilizadas"""

df_organizado = df.drop(['tokenized', 'emojis_tratados', 'regex_tratado', 'sem_abreviacoes', 'no_stopwords'], axis=1)
df_organizado

"""- Mudando o nome da coluna final que foi tratada"""

df_organizado = df_organizado.rename(columns= {'lematizado': 'texto_tratado'})
df_organizado

"""- Reordenando as colunas"""

# Reordenar as colunas
cols = df_organizado.columns.tolist()
cols.insert(1, cols.pop(cols.index('texto_tratado')))
df_arrumado = df_organizado.reindex(columns=cols)
df_arrumado

"""- Fazendo um Label Enconding dos valores da coluna target"""

# Realizando o replace dos valores
replace_dict = {'POSITIVE': 2, 'NEUTRAL': 1, 'NEGATIVE': 0}
df_arrumado['sentimento'] = df_arrumado['sentimento'].replace(replace_dict)
df_arrumado

"""# 5. ‚òï Pipeline do pr√©-processamento

- Salvando o dataset tratado com as features;
- Nesta se√ß√£o, irei implementar o c√≥digo para salvar o dataset como csv, caso seja prefer√≠vel utilizar o dataset em outro lugar.
"""

# df_arrumado.to_csv('dataset_tratado_features', index=False)

"""

---

"""

df_arrumado

"""## 5.1 Defini√ß√£o da Fun√ß√£o de Pipeline

- Fun√ß√£o Pipeline que chama todas as outras fun√ß√µes
"""

def pipeline(comment):
      # Tokeniza√ß√£o
      tokens = tokenizer(comment)
      # Tratamento de Emojis
      demojized = demojize_tokens(tokens, emoji_dict)
      # Remo√ß√£o dos alfanum√©ricos
      no_alfanumericos = removendo_alfanumericos(tokens)
      # Normaliza√ß√£o das abrevia√ß√µes
      normalizado = comentarios_normalizados(no_alfanumericos, dicionario_girias)
      # Remo√ß√£o das stopwords
      no_stopwords = remove_stopwords(normalizado)
      # lematiza√ß√£o
      tratados = lematizacao(no_stopwords)

      return tratados

"""## 5.2 Teste da Fun√ß√£o

- Cria√ß√£o de uma nova coluna chamada 'pipeline' para comprovar que o output est√° igual ao de 'texto_tratado'
"""

df_arrumado['pipeline'] = pipeline(df['texto'])
# Reordenar as colunas
cols = df_arrumado.columns.tolist()
cols.insert(2, cols.pop(cols.index('pipeline')))
df_pipeline = df_arrumado.reindex(columns=cols)
df_pipeline

"""## 5.3 Reset do Dataframe para a forma anterior ao pipeline"""

df = df_pipeline.drop(['pipeline'], axis=1)
df

"""# 6. üìê Vetoriza√ß√£o

- Nesta se√ß√£o iremos aplicar o processo de vetoriza√ß√£o Word2Vec, o qual √© um dos m√©todos estat√≠sticos mais utilizados no pr√©-processamento de dados de texto a fim de uso do PLN (processamento de linguagem natural).
- O objetivo principal do Word2Vec √© mapear palavras em um espa√ßo vetorial de dimens√µes reduzidas, de forma que palavras semanticamente similares sejam representadas por vetores pr√≥ximos. Esse algoritmo √© capaz de capturar rela√ß√µes sem√¢nticas e sint√°ticas entre palavras com base em sua distribui√ß√£o no corpus de treinamento.
"""

df

"""## 6.1 Defini√ß√£o das fun√ß√µes

- Treinar o algoritmo do modelo Word2Vec
"""

# Fun√ß√£o que treina o modelo Word2Vec no corpus do dataframe
def train_word2vec(df, column_name):
    # Obt√©m as frases tokenizadas
    sentences = df[column_name].tolist()

    # Treina o modelo Word2Vec
    model = Word2Vec(sentences, min_count=1) # O argumento "min_count" √© um par√¢metro opcional que indica o n√∫mero m√≠nimo de vezes que uma palavra deve aparecer no corpus para ser considerada no treinamento.

    return model

"""- Criar a tabela de vetores e depois aplicar no dataframe, todos os vetores coletados."""

# Fun√ß√£o que define os vetores para cada palavra do vocabulario
def get_word_vectors(model, sentence):
    vectors = []
    for word in sentence:
        if word in model.wv:
            vectors.append(model.wv[word]) # Append na lista de vetores
    if vectors:
        return np.sum(vectors, axis=0)/len(sentence) # Soma dos vetores para cada frase
    else:
        return np.zeros(model.vector_size)

# Cria√ß√£o do dataframe de vetores para cada frase
def create_word2vec_dataframe(df, column_name, model):
    sentences = df[column_name].tolist()
    vectors = [get_word_vectors(model, sentence) for sentence in sentences] # Itera para cada frase um vetor
    # Cria√ß√£o do dataframe
    df_vectors = pd.DataFrame(vectors, columns=[f"Vetor{i}" for i in range(model.vector_size)])
    df_word2vec = pd.concat([df, df_vectors], axis=1)
    return df_word2vec

"""## 6.2. Teste das fun√ß√µes

- Assim, foram criados 100 vetores para cada frase do index.
- Foram criadas a partir da coluna 'texto_tratado'.
"""

model = train_word2vec(df, 'texto_tratado')
df_word2vec = create_word2vec_dataframe(df,'texto_tratado', model)
df_word2vec

"""# 7. üëæ Modelo"""

df_word2vec

contagem = df_word2vec['sentimento'].value_counts()
print(contagem)

"""## 7.1 Defini√ß√£o da Fun√ß√£o de cria√ß√£o do modelo

- Para o caso atual, estamos utilizando o algoritmo Random Forest
"""

def classification_random_forest(x, y):
  # Dividir o conjunto de dados em treinamento e teste
  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

  # Criar e treinar o modelo Random Forest
  modelo_rf = RandomForestClassifier()
  modelo_rf.fit(x_train, y_train)

  # Fazer previs√µes no conjunto de teste
  y_pred = modelo_rf.predict(x_test)

  # Calcular a acur√°cia
  acuracia = accuracy_score(y_test, y_pred)
  print("Acur√°cia:", acuracia)

  # Calcular o F-score
  fscore = f1_score(y_test, y_pred, average='macro')
  print("F-score:", fscore)

  # Calcular o recall
  recall = recall_score(y_test, y_pred, average='macro')
  print("Recall:", recall)

  # Realizar valida√ß√£o cruzada
  scores = cross_val_score(modelo_rf, x, y, cv=5)  # cv define o n√∫mero de dobras (folds)
  print("Acur√°cia da Valida√ß√£o Cruzada:", np.mean(scores))

  return modelo_rf, y_test, y_pred

"""## 7.2 Balanceamento dos dados

- Aqui fazemos o balanceamento das features devido a descrep√¢ncia de valores entre Neutros e Positivo, em rela√ß√£o aos Negativos.
"""

# Balanceamento dos dados
df_negativo = df_word2vec[df_word2vec['sentimento'] == 0]
df_positive = df_word2vec[df_word2vec['sentimento'] == 1]
df_neutral = df_word2vec[df_word2vec['sentimento'] == 2]

df_positive_resampled = resample(df_positive, replace=True, n_samples=len(df_negativo), random_state=42)
df_neutral_resampled = resample(df_neutral, replace=True, n_samples=len(df_negativo), random_state=42)

dfrnn_balanced = pd.concat([df_negativo, df_positive_resampled, df_neutral_resampled])

"""## 7.3 Separando os dados e testando o modelo

- Aqui fazemos o treinamento do modelo, separando os dados de feature e target.
- J√° apresentamos os valores com as 3 principais m√©tricas e ap√≥s passar por uma valida√ß√£o cruzada.
"""

# Separando os dados em X e y (balanceados)
x = np.array(dfrnn_balanced.drop(['texto', 'texto_tratado', 'sentimento'], axis=1))
y = np.array(dfrnn_balanced['sentimento'])

model_rf, y_test, y_pred = classification_random_forest(x, y)

"""- Tamb√©m apresentamos uma outra visualiza√ß√£o dos resultados.
- Apresentamos o valor de cada classe perante cada m√©trica.
- Al√©m disso, temos a m√©dia dos valores para as 3 m√©tricas aplicadas.


- **OBS**: Lembrando que estamos levando em conta a m√©trica de Recall, pois o nosso objetivo √© acertar mais negativos e abrir espa√ßos para falsos negativos.
"""

classification = classification_report(y_test, y_pred)

print("\nRelat√≥rio de Classifica√ß√£o:")
print(classification)

"""## 7.4 Matriz de Confus√£o

- Plotando a Matriz de Confus√£o:
  - Vertical --> Valor verdadeiro
  - Horizontal --> Valor a ser previsto
"""

# C√°lculo da matriz de confus√£o
cm = confusion_matrix(y_test, y_pred)

# Plot da matriz de confus√£o
fig, ax = plt.subplots()
im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
ax.figure.colorbar(im, ax=ax)

# Configura√ß√µes do gr√°fico
classes = ['Negativo', 'Positivo', 'Neutro']
ax.set(xticks=np.arange(cm.shape[1]),
       yticks=np.arange(cm.shape[0]),
       xticklabels=classes,
       yticklabels=classes,
       title='Matriz de Confus√£o',
       ylabel='Valor Verdadeiro',
       xlabel='Valor Previsto')

# Adiciona os valores nas c√©lulas da matriz
thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(j, i, format(cm[i, j], 'd'),
                ha="center", va="center",
                color="white" if cm[i, j] > thresh else "black")

# Mostra o gr√°fico
plt.show()

"""## 7.5 Salvando como pkl"""

# # Salvando o modelo em um arquivo PKL
# with open('/content/drive/MyDrive/MoÃÅdulo 6/projeto/modelo_rf_novo.pkl', 'wb') as arquivo:
#     pickle.dump(model_rf, arquivo)
# with open('/content/drive/MyDrive/MoÃÅdulo 6/projeto/modelo_rf_novo.pkl_', 'rb') as arquivo:
#     modelo_rf = pickle.load(arquivo)

"""# 8. üí£ Aplica√ß√£o do modelo"""

# Lista de frases positivas
frases = [
    "Essa comida est√° horr√≠vel, n√£o consigo comer.",
    "Eu amo essa m√∫sica!",
    "Que dia maravilhoso!",
    "O jantar estava delicioso!",
    "Que filme terr√≠vel, n√£o recomendo.",
    "N√£o suporto esse tipo de comportamento.",
    "Estou muito feliz com o resultado. üòÅ",
    "Adorei o novo filme do meu ator favorito.",
    "Eu gosto do banco BTG! üòç #btgpactual",
    "Estou decepcionado com o servi√ßo prestado. üò¢",
]

# Criando o DataFrame
dataset = pd.DataFrame({'texto': frases})
dataset

from sklearn.preprocessing import MultiLabelBinarizer

# Criar uma inst√¢ncia do MultiLabelBinarizer
mlb = MultiLabelBinarizer()

dataset['pipeline'] = pipeline(dataset['texto'])

df_vectorized = create_word2vec_dataframe(dataset, 'pipeline', model)

# Transformar as listas de palavras em representa√ß√µes bin√°rias
pipeline_encoded = mlb.fit_transform(df_vectorized['pipeline'])

# Criar um DataFrame com as representa√ß√µes bin√°rias
df_pipeline_encoded = pd.DataFrame(pipeline_encoded, columns=mlb.classes_)

# Concatenar o DataFrame codificado com o DataFrame original
df_vectorized_encoded = pd.concat([df_vectorized, df_pipeline_encoded], axis=1)

# Adiciona uma coluna para cada emoji no DataFrame
for emoji, descricao in emoji_dict.items():
    df_vectorized_encoded[descricao] = df_vectorized_encoded['texto'].apply(lambda x: verifica_emoji(x, emoji))

df_vectorized_encoded

# Realizar a predi√ß√£o usando os dados codificados
df_vectorized_encoded['classificacao'] = model_rf.predict(df_vectorized_encoded.iloc[:, 2:-1].values)

# Exibir o DataFrame resultante
df_vectorized_encoded

